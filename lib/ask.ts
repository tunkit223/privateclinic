import fs from "fs";
import path from "path";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { Ollama } from "@langchain/community/llms/ollama";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { loadQARefineChain } from "langchain/chains";
import { OllamaEmbeddings } from "@langchain/community/embeddings/ollama";
import { PromptTemplate } from "@langchain/core/prompts";

let vectorStorePromise: Promise<MemoryVectorStore> | null = null;
let qaList: { question: string; answer: string }[] = [];

// T·∫£i v√† vector h√≥a d·ªØ li·ªáu t·ª´ JSON
async function loadVectorStore() {
  const jsonPath = path.join(process.cwd(), "data", "faq.json");
  const rawJson = fs.readFileSync(jsonPath, "utf8");
  qaList = JSON.parse(rawJson);

  const documents = qaList.map(({ question, answer }) => ({
    pageContent: `C√¢u h·ªèi: ${question}\nTr·∫£ l·ªùi: ${answer}`,
    metadata: { question },
  }));

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 500,
    chunkOverlap: 50,
  });
  const splitDocs = await splitter.splitDocuments(documents);

  const embeddings = new OllamaEmbeddings({ model: "mistral" });
  return await MemoryVectorStore.fromDocuments(splitDocs, embeddings);
}

export async function askBot(userQuestion: string): Promise<string> {
  try {
    if (!vectorStorePromise) {
      vectorStorePromise = loadVectorStore();
    }

    const vectorStore = await vectorStorePromise;

    // ‚úÖ 1. Ki·ªÉm tra n·∫øu c√¢u h·ªèi tr√πng 100% (b·ªè qua vi·∫øt hoa, kho·∫£ng tr·∫Øng)
    const exactMatch = qaList.find(
      (qa) =>
        qa.question.trim().toLowerCase() === userQuestion.trim().toLowerCase()
    );
    if (exactMatch) {
      return exactMatch.answer;
    }

    // ü§ñ 2. N·∫øu kh√¥ng tr√πng, d√πng AI ƒë·ªÉ t√¨m c√¢u tr·∫£ l·ªùi g·∫ßn ƒë√∫ng
    const relevantDocs = await vectorStore.similaritySearch(userQuestion, 3);
    const model = new Ollama({ model: "mistral", temperature: 0.2 });

    const prompt = PromptTemplate.fromTemplate(`
B·∫°n l√† m·ªôt tr·ª£ l√Ω t∆∞ v·∫•n y t·∫ø chuy√™n nghi·ªáp c·ªßa ph√≤ng kh√°m t∆∞ nh√¢n.
D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë th√¥ng tin t√†i li·ªáu:
{context}

C√¢u h·ªèi: {question}

Vui l√≤ng tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng v√† lu√¥n b·∫±ng **ti·∫øng Vi·ªát**. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu, h√£y l·ªãch s·ª± n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt.
    `);

    const refinePrompt = PromptTemplate.fromTemplate(`
B·∫°n ƒëang c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu sau:
{context}

C√¢u tr·∫£ l·ªùi ban ƒë·∫ßu: {existing_answer}

C√¢u h·ªèi: {question}

H√£y s·ª≠a c√¢u tr·∫£ l·ªùi n·∫øu c·∫ßn, ch·ªâ d·ª±a tr√™n th√¥ng tin ƒë√£ cung c·∫•p. Lu√¥n tr·∫£ l·ªùi b·∫±ng **ti·∫øng Vi·ªát**.
    `);

    const chain = loadQARefineChain(model, {
      questionPrompt: prompt,
      refinePrompt: refinePrompt,
    });

    const result = await chain.call({
      input_documents: relevantDocs,
      question: userQuestion,
    });

    return result.output_text;
  } catch (err) {
    console.error("L·ªói khi g·ªçi askBot:", err);
    return "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau.";
  }
}
